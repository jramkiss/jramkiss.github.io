---
layout: post
title: "ELBO"
date: 2021-10-29 2:22
comments: true
author: "Jonathan Ramkissoon"
math: true
summary: Variational lower bound and the latent variable problem 
---

## Variational Lower Bound

Exact posterior inference is hard in most Bayesian models. Variational inference proposes a way of approximating a posterior over latent variables, $p(z \mid x)$, with a _variational distribution_, $q(z \mid x)$. The idea is that we can specify some family of distributions, $q_{\theta}(.)$ to approximate the true posterior, then learn the distributional parameters, $\theta$ that make $q_{\theta}$ as close as possible to the true posterior. 

#### How to measure closeness? 

One way of measuring closeness in distribution is with [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence). The KL-divergence between the variational distributions, $q$ and the true posterior, $p(Z \mid x)$ is: 

$$ KL(q(Z) \mid \mid p(Z \mid x)) = E_q \left log \frac{q(Z)}{p(Z \mid x)} \right $$

Important to note, is that KL(q \|\| p) $\ne$ KL(p \|\| q) and the KL-divergence is always positive. 

The key idea behind VI is to minimize the KL divergence with respect to the variational paramters, $\theta$. Turns out that we can't do this directly, and instead have to minimize another function that is equal to the KL-divergence up to a constant. 


#### Marginal Likelihood

First we combine Jensen's inequality and the log-marginal likelihood: 


$$
\begin{aligned}
log p(x) &= log \int_Z p(x, z) dz \\
&= log \int_Z p(x, z) \frac{q(z)}{q(z)} dz \\
&= log E_q [ \frac{p(x, z)}{q(z)} ] \\
\end{aligned}
$$


From Jensen's inequality: $f(E(X)) \ge E(f(X))$:

$$
\begin{aligned}
log p(x) \ge E_q \left log \frac{p(x, z)}{q(z)} \right 
log p(x) \ge E_q \left log p(x, z) \right - E_q \left log q(z) \right
\end{aligned}
$$

This is the ELBO. 

#### KL-Divergence (again)

$$ KL(q(Z) \mid \mid p(Z \mid x)) = E_q \left log \frac{q(Z)}{p(Z \mid x)} \right $$

$$ = 





## Aside: Latent Variable Problem 

A latent variable is a quantity that we don't directly observe. This arises for different reasons. It could be because of observational noise or that the quantity doesn't really exist, for example the clusters in a GMM. Lots of interesting problem formulations contain latent variables which brings some challenges during inference. 

In models like this, if $Z$ was our observed data, then we can use the likelihood function to do inference on $\theta$ and walk away happily. However, $X$ is observed and only connected to $\theta$ through $Z$. 

The model is: 

$$ Z_j \sim f(z \mid \theta), \qquad j = 1, ..., m $$

$$ X_i \sim g(x \mid Z), \qquad i = 1, ..., n $$

In order to do inference on $\theta$, we need the likelihood function, $L(\theta \mid X)$, which we can only get by introducing $Z$ and integrating it out. 

$$ L(\theta \mid X) \propto f(x \mid \theta) = \int p(X, Z \mid \theta) dZ $$

$$ \qquad = \int p(X \mid Z) p(Z \mid \theta) dZ $$


This integral is almost definitely intractable, which leads into the main idea of this blog post: variational lower bounds. 