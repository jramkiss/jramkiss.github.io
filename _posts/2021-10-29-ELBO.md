---
layout: post
title: "Variational Lower Bound"
date: 2021-10-28 2:22
comments: true
author: "Jonathan Ramkissoon"
math: true
summary: Variational (evidence) lower bound and the latent variable problem 
---

## Variational Lower Bound

Exact posterior inference is hard in most Bayesian models. Variational inference proposes a way of approximating a posterior over latent variables, $p(z \mid x)$, with a _variational distribution_, $q(z \mid x)$. The idea is that we can specify some family of distributions, $q_{\theta}(.)$ to approximate the true posterior, then learn the distributional parameters, $\theta$ that make $q_{\theta}$ as close as possible to the true posterior. 

#### How to measure closeness? 

One way of measuring closeness in distribution is with [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence). The KL-divergence between the variational distributions, $q$ and the true posterior, $p(Z \mid x)$ is: 

$$ KL(q(Z) \mid \mid p(Z \mid x)) = E_q \left[ log \frac{q(Z)}{p(Z \mid x)} \right] $$

Important to note, is that KL(q \|\| p) $\ne$ KL(p \|\| q) and the KL-divergence is non-negative. 

The key idea behind VI is to minimize the KL divergence with respect to the variational paramters, $\theta$. Turns out that we can't do this directly, and instead have to minimize another function that is equal to the KL-divergence up to a constant. 


#### Marginal Likelihood

First we combine Jensen's inequality and the log-marginal likelihood: 


$$
\begin{aligned}
\log p(x) &= \log \int_Z p(x, z) dz \\
&= \log \int_Z p(x, z) \frac{q(z)}{q(z)} dz \\
&= \log E_q \left[ \frac{p(x, z)}{q(z)} \right] \\
\end{aligned}
$$


From Jensen's inequality: $f(E(X)) \ge E(f(X))$:

$$
\begin{aligned}
\log p(x) \ge E_q \left[ \log \frac{p(x, z)}{q(z)} \right] \\
\log p(x) \ge E_q \left[ \log p(x, z) \right] - E_q \left[ \log q(z) \right]
\end{aligned}
$$

$p(x)$ is typically referred to as the evidence, so I believe that's why this bound is called the evidence lower bound or ELBO. 

&nbsp;

#### KL-Divergence (again)

$$ 
\begin{aligned}
KL(q(Z) \mid \mid p(Z \mid x)) &= E_q \left[ \log \frac{q(Z)}{p(Z \mid x)} \right] \\
&= E_q \left[ \log q(Z) - \log p(Z \mid x) \right] \\
&= E_q \left[ \log q(Z) \right] - E_q \left[ \log p(Z \mid x) \right] + \log p(x) \\
&= - \left( E_q \left[ \log p(Z \mid x) \right] - E_q \left[ \log q(Z) \right] \right) + \log p(x)
\end{aligned}
$$

So we have decomposed the KL-divergence between the posterior and the variational distribution into the log marginal likelihood plus the ELBO. This is cool. In order to minimize the KL-divergence, we can maximize the evidence lower bound with respect to $\theta$, which usually requires us to decompose the ELBO further. 

&nbsp;

Some really useful resources for this are: 

- [David Blei's VI lecture](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)
- [Original VAE paper](https://arxiv.org/pdf/1312.6114.pdf)


&nbsp;

### Aside: Latent Variable Problem 

A latent variable is a quantity that we don't directly observe. This arises for different reasons. It could be because of observational noise or that the quantity doesn't really exist, for example the clusters in a GMM. Lots of interesting problem formulations contain latent variables which brings some challenges during inference. 

In models like this, if $Z$ was our observed data, then we can use the likelihood function to do inference on $\theta$ and walk away happily. However, $X$ is observed and only connected to $\theta$ through $Z$. 

The model is: 

$$ Z_j \sim f(z \mid \theta), \qquad j = 1, ..., m $$

$$ X_i \sim g(x \mid Z), \qquad i = 1, ..., n $$

In order to do inference on $\theta$, we need the likelihood function, $L(\theta \mid X)$, which we can only get by introducing $Z$ and integrating it out. 

$$ L(\theta \mid X) \propto f(x \mid \theta) = \int_Z p(X, Z \mid \theta) dZ $$

$$ \qquad = \int_Z p(X \mid Z) p(Z \mid \theta) dZ $$


This integral is almost definitely intractable, which leads into the main idea of this blog post: variational lower bounds. 