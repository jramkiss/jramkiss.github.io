---
layout: post
title: "Attention as an Expressive Feed-Forward Layer"
date: 2023-03-11 2:22
comments: true
author: "Jonathan Ramkissoon"
math: true
summary: A post viewing the attention mechanism as a more expressive feed-forward layer
---

Question: 
    - Can we use a self-attention layer in place of a feed-forward layer to model dependency more expressively in non-sequential input?
    - If so, can we use the self-attention to provide interpretations of predictions?