---
layout: post
title: "Sequential Monte Carlo for Dummies"
date: 2021-09-29 2:22
comments: true
author: "Jonathan Ramkissoon"
math: true
summary: This post is an introduction to sequential Monte Carlo via the bootstrap particle filter, for anyone as dumb as I am 
---

Sequential Monte Carlo is a general class of methods designed to sample sequentially from a sequence of target densities. State-space models is an example of a class of models that naturally have a sequence of target densities, if we're interested in latent-state estimation.

For me, Sequential Monte Carlo is the epitome of "the math is confusing but the idea is simple", so this post will take a stab at distilling the idea with math and code. 


## Overall Problem 

I find that the combination of math and code makes algorithms much easier to understand, so here's the math. I'll mainly follow the notation from [this](https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf) paper, but will only write down the neccessities. 

The latent states, $X_t$ are modelled as a Markov process with initial state $p(x_0)$ and transition equation $p(x_t \mid x_{t-1})$. The observed data, $Y_t$, is assumed to be conditionally independent given the hidden state at time $t$, $X_t$, and has density $p(Y_t \mid X_t)$. The model is completely specified by the following: 

$$
\begin{aligned}
X_0 \sim p(x_0) & \qquad \qquad \text{Initial state} \\
p(X_t \mid X_{t-1}) &  \qquad \qquad \text{Transition density} \\
p(Y_t \mid X_t) & \qquad \qquad \text{Marginal of $Y_t \mid X_t$?}
\end{aligned}
$$

The aim is to estimate the posterior distribution of the latent states, $p(X_t \mid Y_t)$, which we use the bootstrap particle filter for. 


## Bootstrap Particle Filter 

The key idea behind particle filtering comes from importance sampling. In importance sampling, we empirically approximate a distribution, $p(x \mid y)$ by sampling from a proposal distribution, $q(x \mid y)$ and re-weighting the samples according to: 

$$ 

In the bootstrap particle filter, we use the transition density as the proposal density in the importance samling step.

Model:

$$ x_t = \frac{1}{2}x_{t-1} + \frac{25x_{t-1}}{1 + x_{t-1}^2} + 8cos(1.2t) + v_t $$

$$ y_t = \frac{x_t^2}{20} + w_t $$

$$ x_0 \sim N(0, \sigma_1^2) $$

$$ w_t \sim N(0, 1) $$

$$ v_k \sim N(0, \sigma_k^2) $$


Outline of SMC algorithm: 

* Initialization: 
    * For $i = 1, ..., N$, sample $x_0^{(i)} \sim p(x_0)$ and set $t = 1$.
    
* Importance sampling step: 
    * For $i = 1, ..., N$, sample $x_t \sim p(x_t \mid x_{t-1}^{(i)})$ and inlcude in $x_{0:t}$
    * For $i = 1, ..., N$, evaluate the importance weights:
    $$ w_t^{(i)} = p(y_t \mid x_t^{(i)}) $$
    * Normalize importance weights
    
* Selection step: 
    * Resample with replacement $N$ particles $(x_{0:t}^{(i)}, i = 1, ..., N)$ according to $w_t^{(i)}$


## Questions to answer

- Setup a simple bootstrap filter with the math of a latent variable model 
- How to deal with particle degeneracy and impoverishment? When I run the resampling step, the same particles are resampled, so we still end up with just 1 particle

#### Bonus 

- How is parameter inference done with SMC



## Importance Sampling

- https://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf
- https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2014-15/lecture/2015/03/10/notes-lecture6.html
- https://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/


## SMC

- [OG SMC tutorial](https://www.cs.ubc.ca/~arnaud/doucet_johansen_tutorialPF.pdf)
- https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2014-15/lecture/2015/03/17/notes-lecture8.html
- https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2014-15/lecture/2015/03/15/notes-lecture7.html
- https://umbertopicchini.wordpress.com/2016/10/19/sequential-monte-carlo-bootstrap-filter/


